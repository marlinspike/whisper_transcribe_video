Hello, and welcome to Clearer Thinking with Spencer Greenberg, the podcast about ideas that matter. I'm Josh Castle, the producer of the podcast, and I'm so glad you joined us today. In this episode, Spencer speaks with Ilya Sutskever about the nature of neural networks, the psychology and sociology of machine learning, and the increasing power of AI. Ilya, welcome. Thank you. It's good to be here. You've really been a pioneer in the advancement of AI. So I'm really excited to talk to you today about where AI has been, you know, what kind of progress we've seen, where AI is going, and kind of your thoughts for the future as well. Yeah. Happy to talk about it. Great. So just to start off on a bit of a philosophical note, you know, people sometimes debate whether machines today, while doing these things that seem intelligent, whether they're really intelligent. So I just wanted to ask to start, how do you think about what intelligence means? Intelligence is a little tricky to define, but I think there are two useful ways to think about it. One useful way of thinking about intelligence is by saying, well, we are not sure what intelligence is, but human beings are intelligent. And let's look at the things that human beings can do. And if you have computers, which can do some of those same things, it will mean that those computers are intelligent. You can also try to come up with formal definitions of intelligence. For example, a system is intelligent if it can do really well on a certain broad range of tasks specified in a certain formal way. But I find those definitions to be a bit less useful. I find just thinking about things people or animals, things that people primarily can do. And if computers can do those things too, then these computers are intelligent. And the more of those things computers can do, the more intelligent they are. One thing that's been fascinating to me in the history of AI and machine learning is how so many tasks have existed where people said, oh, a computer is never going to be able to do that. Or they say, if a computer could do that, it could do anything a human can do. And yet again and again and again, we seem to be able to get computers to do the thing, despite many people's sort of expectations that there's no way it could do that, or that by doing that, it must be able to do everything. And so it's sort of like, it seems like many more tasks are narrow than people realize. And I'm curious if you have a perspective on that. Yeah. I mean, I think what's happening here is that our intuitions about intelligence are not exactly perfect. Intelligence is not something that's very easy to fully observe and to understand empirically as evidenced by the examples that you brought up. I think people have intuitions where certain tasks feel hard for them. And so they feel that it would be really impossible for a computer to solve those tasks without solving everything else. I mean, I think on the flip side, people did say things like, well, if you can play chess, then you can do all these other amazing things. And it turns out that those tasks have indeed been quite narrow. But I'd say that with the situation we are today with deep learning, we do have quite general purpose tools. And if you want to get really good results on some task, if you can collect a large amount of data, you will, in fact, get very good results on this task. Yeah. One of the things that most impressed me about GPT-3, a system that I know you were really integral in creating, is that while you trained it to do just one thing, it was able to do many things as a consequence. And just for those listeners that kind of don't know too much about GPT-3, maybe you could just start by telling the listener a little bit about what it is, and then we could talk about sort of how it's able to generalize across tasks. Yeah. So to explain what GPT-3 is, it will be helpful to understand what neural networks are and how they are the foundation of what GPT-3 came out to be. So the way to think about a neural network is that it's a certain kind of a parallel computer which can program itself. That's what a neural network is. It's a parallel computer with a certain amount of memory and a certain amount of sequential, you know, it has a lot of parallel compute and a limited amount of sequential compute. And the thing that's special about this computer is that it has a learning algorithm. It has a way of automatically programming itself. So the reason why I describe neural networks as these restricted computers in this case is because it becomes a lot easier to understand the idea that, okay, if a neural network is like this computer which can program itself, it can actually do a whole lot. And the next question that arises immediately is, so let's suppose it is true that we have these neural networks, and the larger they are, the more powerful the computer they implement. The question we need to ask then is, what should we ask the neural network to learn? Because okay, you have this capability, which way should you apply it to? Like you say, okay, the neural network can be trained, i.e. that computer which is implemented by the neural network can be programmed by the procedure. You feed it some inputs, you see how it behaves, and you say, ah, this is not the behavior which I want, please do this other thing. And the neural network will say, okay, I get it. I'm going to modify myself to not make this mistake in the future. So the real insight of GPT-3 is that there is one task which, if you get a neural network to be really, really good at it, will give you as a byproduct all kinds of other capabilities and tasks which are very interesting and meaningful to us. The task that I'm talking about is doing a really good job at guessing the next word in a corpus of text. So the way it would be set up is that you have a neural network and you give it some text and then you ask it to guess the next word. And the way guessing the next word works is that you output probabilities about what the next word should be. So you say, well, maybe the next word is this one, or maybe that one. And you want to place your bets such that in general, you are correctly confident and your predictions are generally correct. Sometimes you can narrow down your predictions by a whole lot, sometimes not so much. So let's recap where we are. We have these neural networks, which are these parallel computers, which if you make them large, they can learn all kinds of stuff. We can then point them to the task of predicting the next word, guessing the next word in a large corpus of text really well. Now the third thing I want to talk about is the implication of this capability. Suppose you have a system which gives it some text and it guesses the next word pretty well. Then you just take the guess and you feed it back to the neural network and you do it again. And this way you generate text. And now you have a system which can respond in meaningful text to any other meaningful text. And this ability hides within itself lots of abilities that we want. Like you say, hey, like what do people do in text as expressed on the internet? Well, sometimes they might summarize text, sometimes they might converse in text. They're all the things that GPT-3 does are in some sense like a reflection and interpolation of things that people do in text that's expressed online. Right. So even though you never program the system to write poetry or translate languages or do simple math problems, it learns how to do all of those things because if you are going to predict what's going to come next.If you start with the beginning of a poem, the most likely thing is going to be the end of a poem, right? If you start with a math problem, the most likely thing that's going to come next is the answer and so on. So in order to predict, it essentially has to learn all these different subtasks. That's exactly right. And I want to add another thing, which is like at the level of tasks we are talking about, directly programming a system to do these tasks is basically like really impossible. The only way to get capabilities like this into a computer is by building a neural network inside the computer and then training it on a task like this, which is something like guessing the next word. So here with GPT-3, we're trying to predict the next word. What's the connection between predicting and understanding? So there is an intuitive argument that can be made that if you have some system which can guess what comes next in text or maybe in some other modality really, really well, then in order to do so, it must have a real degree of understanding. Here is an example, which I think is convincing here. So let's suppose that you have consumed a mystery novel and you are at the last page of the novel and somewhere on the last page, there is a sentence where the detective is about to announce the identity of whoever committed the crime. And then there is this one word which is the name of whoever did it. At that point, the neural net, the system will make a guess of the next word. If the system is really, really good, it will have a good guess about that name. It might narrow it down to three choices or two choices. And if the neural network has paid really close attention, well, certainly that's how it works for people. If you pay really close attention in a mystery novel and you think about it a lot, you can guess who did it at the end. So this suggests that if a neural network would do a really good job with predicting the next word, including this word, then it would suggest that it understood something very significant about the novel. You cannot guess what the detective will say at the end of the book without really going deep into the meaning of the novel. And this is the link between prediction and understanding, or at least this is an intuitive link between those two. Right, so the better you understand the whole mystery novel, the more ability you have to predict the next word. And so essentially understanding prediction are sort of two sides of the same coin. That's right, with one important caveat, or rather there is one note here. Understanding is a bit of a nebulous concept. What does it mean if a system understands one concept or doesn't? It's a bit hard to answer that question, but it is very easy to measure whether a neural network correctly guesses the next word in some large corpus of text. So while you have this nebulous concept that you care about, you don't have necessarily a direct handle on it. You have a very direct handle on this other concept of how well is your neural network predicting text and you can do things to improve this metric. So it sort of operationalizes understanding in a way that we can actually optimize for. Precisely. Now, as an aside, I'll just tell the listener, if you've never seen GP3 in action, I actually recorded a podcast episode where I interview GP3 and have it play different characters. So I'd suggest you go check that out and you might want to come back to this conversation after that, just so you can see what it's actually like. So my next question for you, what was the barrier, let's say five years ago, to building something like GP3 and how did you overcome that barrier? So I think three things needed to come together. GP3 is not a small neural network in terms of the amount of compute that was used. And one very direct barrier was to simply have the compute in place. That means both having really fast GPUs, having access to a large cluster, having the infrastructure and the techniques for utilizing the large cluster to train a single large neural network. This was the first obstacle. The second obstacle to actually successfully use this compute, you need to have a neural network architecture and the optimization tools to successfully train it. In other words, you need to have a neural network architecture, such that if you were to apply this compute to it, you would actually get good results. That has also not been the case in the past. I guess five years ago, things would have been quite a bit worse. We didn't even have the transformer back then. So we would not have been able to make as anywhere as efficient use of the compute that we had. And the third obstacle was to realize that this is a good idea. So the thing about deep learning is that you have this strange phenomena where in theory, if you train larger neural networks and bigger datasets, they should get more amazing results. But it's something that's not easy to believe in. And just believing in that is a major, major part of the advance that led to GPT-3. So let's go back through those three points because I think each of them has really interesting stuff there. So point one, you mentioned about compute and infrastructure. Could you give people some idea of just how much compute or infrastructures needed to train a system like GPT-3? On the compute, you need to use thousands of GPUs for quite a while, for at least a number of weeks in order to get the result that you need. So it's just a very large amount of compute and it's not easy to get it. And with Moore's law, I'm guessing that this would have essentially been impossible five years ago to do this much compute or it would have been just so obscenely expensive that it wouldn't have been reasonable? It would have been more expensive. That would definitely be true. But in addition, I think the transformers came out in late 2017. So maybe five years ago, just to explain what the transformer is and to talk about what deep neural networks are and what's the deal there, right? So a neural network is a circuit and a circuit is best thought of, in my opinion, as a parallel computer. Because the neural network has a learning algorithm, the parallel computer that's implemented by the neural network can program itself from data. Now, various attributes of the neural network determine the shape of the computation that's implemented by it. So if you have a shallow neural network, it means that you have a parallel computer which can only do one step in parallel. That doesn't seem very powerful and that's actually totally obvious. If you have a parallel computer, but it only has one step of parallel computing, it won't be able to do much at all. This is basically what shallow models in AI are like. You can prove mathematical theorems about, hey, it's shallow so it's easy to handle mathematically so I can prove that my learning algorithm will find the best function in my shallow function class. But if you think of it from the perspective of what computer is being implemented by my functions, it's like a parallel computer that does only one or like a very, very small number of steps. Once you add deep networks, the number of steps that's afforded to the parallel computer increases significantly and because of it, it can do a lot more. The transformer then takes this to the next level but the main usefulness of the transformer comes from what.may appear to be a technicality at a sufficiently high level, like the conversation that we are having. The technicality is this. If you think about neural networks as you describe them, where you have these matrix multiplies followed by element-wise nonlinearities, it is very natural to apply them to vectors, but it becomes clunky to apply them to sequences of vectors. And most interesting data, or a great deal of very interesting data, comes in the form of sequences, language as an example. And so you say, okay, well, I want to feed, I want my neural network to process long sequences of vectors. How to do that? And then you start saying, okay, well, maybe we can apply a neural network in one way, or maybe we can apply a neural network in another way. And the dominant way of applying neural networks to sequences was this way called the recurrent neural network. I'm not going to explain to you what it is because it would be too much of a digression, but it's a very, very beautiful and elegant neural network architecture design. It's very attractive, but it had a problem that it was difficult to train well. So it would not be able to process sequences as well as we hoped. Now, back then we didn't know that something better was possible. Then the main innovation of the transformer is that it lets you process long sequences of vectors in a very compute-efficient way, and most crucially, in a way that's easy to learn for the learning algorithm. And this is a point which I haven't elaborated on, but I think it's important that I do so now. I mentioned that neural networks come with, that are like parallel computers, which have a way of programming themselves automatically from data. But their learning algorithm is finicky. It doesn't work under all conditions. And it's not easy to tell, especially a priori, it especially wasn't easy to tell back then when precisely it would work and when it wouldn't, and what would be the way to fix it. And so you can see a great deal of the breakthroughs that were done in AI was to figure out, to hone in on the conditions where the learning algorithm, which programs the neural networks, which is called the backpropagation algorithm, the conditions under which it works reliably and well. And the transformer is an architecture with the following three properties. It can process long sequences of vectors very naturally. It is very comparatively straightforward to learn with the backpropagation algorithm. And in addition, it has one other big advantage. It runs really fast on GPUs. And GPUs is the main way in which we implement neural networks. So the fact that something runs quickly on GPUs is a huge advantage. So those were the three things that made the transformer really, really successful. So in some sense, you can think of transformers as giving us the power to better leverage the computation that we already have. Is that right? In one sentence, it is right. But in another sentence, it gives us something even more than that. If you care about processing sequences, and for example, you know, we spoke about GPT-3, GPT-3 processes a sequence of 2000 words to make its prediction about the next word. Back in the day, it was a lot of words. By comparison, like if you were to look at recurrent neural networks, the most anyone has ever gone successfully was like 100 words. So it's not only that we leverage computer more efficiently, it's that their optimization problem is much easier than the problem of the previous best approach for processing sequences. So that training a recurrent neural network to process a sequence of 2000 words would just not be a meaningful thing. It would not benefit from these extra words. Do you see what I mean? Right, so with GPT-3, you could give it a whole essay that's 2000 words long and say, well, what's the next word in the essay? Whereas these older models, you were limited to such a small input that it sort of limited the capabilities of the system. That's right. Now, in fairness, there have been a lot of advances in optimization in machine learning and our understanding of what makes architectures of neural network architectures work is far greater than it used to be. So it is very possible that with a little bit of work, it will be possible to bring back recurrent neural networks and make them competitive with transformers. But it seems unnecessary because transformers have so much going for them and they work so well with GPUs. So we've got this really good thing going and I don't think we are hitting the limitations of our approaches just yet. Now, you mentioned that these are really good for processing sequences of vectors. And I just want to explain that briefly for those that might be confused about what that means here. So as I understand it, a word or really a token, which is generally a piece of a word, is going to be represented as a vector. And so if you're processing 2000 words, one way to think about that is you're really processing a sequence of vectors. So essentially a vector being a list of numbers where each token in English is just represented a list of numbers. And then the sequence of them is the set of all the vectors for all the different words. Is that correct? Yes, that's right. Sequences are just very natural. They occur all over. Sequences of words, like language is sequential, speech is sequential. Okay, so this brings us to the third point you made about why it would have been hard to build something like GPT-3 five years ago, which is just this belief in large networks. So I'd be really interested to hear your thoughts on that. Why do people not believe that large networks would give them such a big advantage? And why was it that you and your colleagues at OpenAI thought otherwise? So the second question is much harder to answer. But the first question, I think the answer has to do with psychology. I think there are very powerful psychological forces in play where if you work a lot with a particular system, you can feel its limitations so keenly. And your intuition just screams that there are all those things that whatever you're doing cannot do. And I think that this is why researchers in AI have consistently underestimated neural networks. Especially in hindsight, it is so clear that, of course, neural networks can do a lot of things. But I think even now there are plenty of people who will tell you that you need to, for example, combine neural networks with symbolic approaches because of inherent limitations to neural networks. So I think this belief prevails in some circles even today. I've heard you mention elsewhere that there may also be an issue with the way that machine learning algorithms were measured. Essentially, the academics would use fixed data sets where they'd say, here's the benchmark, see how well you can perform on that. Do you want to unpack that a little bit? Yeah, that's right. This is a really correct point. So I mentioned the psychology of machine learning, but I forgot to mention the sociology of machine learning. Indeed, the way research has been done, especially 5, 10, 15 years ago, 20 years ago, is that researchers were primarily interested in developing the great machine learning algorithm, but they were very uninterested in building data sets. And the reason for that is that building data sets is not intellectually stimulating. I think that's all there is to it. Because of that, because building data sets is not intellectually stimulating, researchers would just use the data sets that have existed primarily. They would just say, oh, here is a well-established data set.Let us now try to get better results on this dataset. If you are in this regime, then if you try to increase the size of your neural network, you will not get better results or much better results because for the larger neural network to get better results, it also needs more data. It makes intuitive sense if you think about it because the larger neural network has more trainable parameters. It has more synapses. And by the way, like the way you should think about the parameters of a neural network is as of the strength of the connections between its neurons. The bigger the neural network, the more neurons, the more connections there are between the neurons. And the more data you need to constrain all these connections. Again, these things are obvious in hindsight, but in fairness, pretty much every really significant advance in at least AI has been fairly obvious in hindsight. This reminds me of The Bitter Lesson, which is a short essay written by Rich Sutton. Have you read that essay? Yes. I'll just read a very brief excerpt from it because I think it's super interesting. Sutton says, the biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective and by a large margin. Seeking an improvement that makes a difference in the short term, researchers seek to leverage their human knowledge of the domain. But the only thing that matters in the long run is the leveraging of computation. And so I think the point of this essay is that researchers keep trying to build better AIs by using their cleverness, their knowledge of the domain that the AI is trying to solve and maybe for a year or two, that does produce better results, but then someone ends up blowing past them by throwing more computation and more data at the same problem, but with less clever methods. Yeah, that's definitely borne out in neural networks research in deep learning research. Researchers want to make progress and researchers want to apply their skills to make progress. And oftentimes it is easy and satisfying in the short term, exactly as you said, to say, hey, what if we change things up a little bit in a certain way and performance improves? And most of those changes lead to improvements on a specific task, but they don't generalize. And you can potentially spend a very large amount of human effort baking in those improvements into the system. There's a certain flavor, there's a certain taste to improvements in methods that actually stand the test of time and that work better as you have more compute. Those improvements exist, and we mentioned, you know, we discussed some of them. The switch from recurrent neural networks to the transformer was one, because it is still an improvement that researchers have produced, but it leads to better utilization of compute and unlocking of new capabilities. Obviously, improvements in optimization, which is something that we discussed as well, is another category of such improvements, and there's a whole bunch of others. So I don't want to overstate the message of beta lesson that the only thing that matters are simple methods. What I think is true is that what the only thing that matters are really good, simple methods that scale. Are you facing a tough or important life decision? Then you should try using Clearer Thinking's Decision Advisor tool to make it easier. With this tool, you won't have to stress as much about those big life decisions. The Decision Advisor can walk you through even the most complicated situations in minutes, so you can come out on the other side with a better idea of what to do. To use the free Decision Advisor tool or to find Clearer Thinking's other free tools and mini-courses, head to clearerthinking.org. So now that we're in this era where these massive networks are used in order to get state-of-the-art predictions, is academia able to keep up? I mean, it can cost millions of dollars to train one of these networks, and so I wonder if we're going to get to the point where you have to be part of a group like OpenAI or DeepMind to really be at the cutting edge. Yeah, I think the situation for academia is such that academia will no longer be able to be at the cutting edge of training the absolutely largest models. I think something similar is happening in systems research, and something similar has happened in semiconductor research as well. There has been a time where the most cutting-edge research in distributed systems has been taking place in academia. Now it's taking place in companies like Google and maybe other companies. There is still interesting academic research going on that studies questions which are neglected by these big companies. And with respect to AI, I can see things going in two ways for academia. I think first is that they could produce a lot of foundational understanding of the methods that we use. And I would also expect there to be a lot of collaboration between academia and some of the companies that you mentioned, like OpenAI or DeepMind or Google, because these companies often expose the models that they train through various APIs, and academics and people in academia could study their properties and modify them in various ways and see what happens and discover new and useful things. Got it. So you think there still could be a helpful role for academics pushing forward the state-of-the-art, even if they can't actually train the state-of-the-art models? That's right. It will be a different kind of work, and there will be some people who will find it very interesting and attractive, and they will work on it. Now, when we talk about these large models, with GPT-3, we're talking about 175 billion parameters, at least in the original version of GPT-3. I don't know whether that's changed. And I think it's such a shockingly large number of things being learned, of essentially numbers in the model being learned. I think a lot of people who, let's say, have some statistics background might think, well, isn't this just going to overfit like crazy? And so to get an intuition for overfitting, imagine you're just doing a simple linear regression and you have a scatterplot with 10 points. You could fit a line through those points, but you could also just take a pencil and draw some crazy squiggly line that hits every single point perfectly. But there's some intuition people have that if you draw some arbitrary squiggly line that hits every point perfectly, that's not going to generalize very well the new data. Whereas if you fit a smooth line through the points, that's probably going to generalize better. And when you have 175 billion parameters, I think a lot of people who have some statistics under their belt, their intuition is going to be, how can this learn anything, right? Isn't this just going to immediately overfit? Yeah, that's a very deep question. And this question has indeed perplexed a lot of researchers. And I think right now we have pretty decent answers to it. There are actually two parts to what I'm going to say. The first question that you brought up was GPT-3 specifically. It has so many parameters, 175 billion parameters, which is a lot. However, it was trained on 300 billion tokens. So it was actually trained on more data than parameters. And so you could argue that even if we were in the regime that the statistics people are used to, for something like GPT-3, the situation wouldn't be too bad because the amount of data has been quite large compared to the parameters. However, there is another phenomenon that researchers have observed, which I think is what you are truly.are alluding to, that even when you train a neural network, which has far more parameters than it has training points, and that's often the case until recently, has been the case for various vision classification tasks, because even though you feed it an entire image, you only learn from a small label on the output. So you have neural networks with far more parameters than labels. How can those generalize? And there, it turns out that this happens due to properties of the optimization algorithm of stochastic gradient descent, where there are various arguments can be made around the stochastic gradient descent, finding solutions that have the smallest norm naturally and automatically. And there are other arguments that show, that argue, I would say with some degree of convinceness, that stochastic gradient descent in particular has an effect where it removes the information from the parameters. It minimizes a certain measure of information of the parameters, which is distinct from their actual numerical quantity. So to sum it up, even if those details may not have been the easiest to grasp immediately, it is the property of the training procedure that makes it so that even when you have a huge neural network with a huge number of parameters that vastly exceeds the amount of data that you have, you still achieve good generalization. Right, so maybe an intuition that will help people here is that if the model could really take on any value for any of those parameters, then it probably would overfit. You know, it would be predicting the noise, not just the signal. But the training procedure actually puts in a preference for some of those primary values over others. So even though all those parameters technically exist, it's sort of more constrained than it seems. And one way to think of it is essentially sort of has a prior of preferring sort of certain solutions to others. Is that a fair way to put it? That's a fair way to put it, yes. If you had some adversary that would say, I can absolutely very easily produce a neural network that's going to do perfectly well on the training set and do horrendously poorly on the test set. So not only such adversaries exist, but you can even like, if you try really hard, it is possible to actually build, program one such learning algorithm that will have deliberately, intentionally poor generalization properties. Of course, it's not something we'd ever want to use, but yes, the learning algorithm does a lot of the heavy lifting in the success of these neural networks, not only in training, but also in generalization, the real life performance. One thing that surprises me when I'm using GP3 sometimes is that it seems to know exact texts in some cases. Like if you give it the beginning of like a really famous book, it might be able to actually write the exact correct sentences that are actually in that book. But a lot of times it will also generalize. It will say things have never been said. You can Google the phrases it comes up with and they've never been said by a human, at least not on a webpage that's available and indexable by Google. So I'm curious, like, maybe this is hard to answer, but in some sense it is memorizing some of its training input, but it's also generalizing. And do you have anything to say about that? Yes, I do. I think it's a very profound and deep question. It touches on the question of where does memorization end and generalization begin? And I think that it's like they go hand in hand to a significant degree. And there is an intuition from Bayesian reasoning. You can construct an intuition about how this memorization is fully compatible with generalization by considering a Bayesian example. So suppose you have your prior distribution over some space of functions. And suppose that I tell you, hey, on this input, you have some output. Please give me the posterior distribution on the functions that all of whom satisfy that on this input they have this output. You could say, no problem, here's your Bayesian posterior. You could then use it to make really good predictions. If your prior distribution has been over an interesting set of functions, this posterior will give you great generalization. You can do it for several data points and you'll get a posterior distribution over all the functions, which perfectly hit their desired outputs on the training data. And yet you'll generalize well. So this, what I'm trying to say here is that simple formulations of Bayesian inference are compatible with memorization and generalization simultaneously because this posterior distribution over functions will have the property that every function in it memorizes the training set perfectly. Yet when you average over this posterior, you will get good predictions. Do you see what I mean? Right, so you can essentially, if we think of the neural net as learning many, many, many functions internally, you can have some of those functions that are essentially memorizing data. So by memorizing, we mean really like for a given input, they'll produce the exact output. Like you give it the first sentence of a book, it will produce the next sentence, right? Whereas because it's combining all of these functions together, it's still able to generalize to new circumstances despite having these many pieces that are kind of exact memorizers. Is that right? This is close, but not exactly right. So the example with the idealized Bayesian inference is not something that the neural network literally does. The neural network does something that tries to approximate it, but in a way that's a bit different that I don't want to get into. The reason I brought up the Bayesian inference example is because it is relatively widely known that Bayesian inference gives you correct generalization. If you follow the recipe of Bayesian inference, which is down to the dot, which is computationally intractable precisely, but suppose you did that, you would get very good predictions. Yet, if you say, okay, I have my class of functions, all my neural networks together, rather than sub-neural networks, like every function is a different configurational weights, all my neural networks. And I say, here's my training set. Please give me the posterior distribution over all the neural networks, which satisfy this training set. So now I'm going to have a posterior distribution over neural networks, each of which memorize the dataset perfectly. Yet, when you make Bayesian predictions, when you average over these neural networks, they'll be really good. So the high, like, I feel like I got, I feel like the details may be a little hard to follow, but they're not important. The important thing is this. It appears to be perplexing. How can it be that a neural network memorizes all this data and generalizes at the same time? Yet, I claim that idealized Bayesian inference, which is known to make the best predictions in a certain sense, the best predictions possible in a certain sense, has the same property. If you say, here's my training set, you'll get a Bayesian posterior over functions, which memorize it perfectly, yet you will get good generalization. So Bayesian inference also exhibits the same. So maybe we're confused about this idea that memorization is a bad thing, just because in early AI systems, it happened to often show that you had not trained properly, but it's not inherently a problem. Yeah, I think that's right. I wouldn't say it's because of the early AI systems. I would say the reason is different. I think this is one area where humans and the human brain operates differently from our neural networks. I think our subjective experience is that memorization is something that we really don't like to do.generally speaking. At least in my personal experience, I know I don't like memorizing things. I find it difficult and uncomfortable. And so perhaps we can do better. Perhaps it is possible to do better than just memorizing everything. But my statement is different. My statement is that the gold standard formalism of generalization, or at least a gold standard formalism, Bayesian inference, is very consistent with you memorize everything, yet you fully generalize. That's really interesting. So we've talked about how these systems work. I want to now switch and talk about the future of these systems and where you think they're going. One thing that comes up as soon as you start thinking about the future of these systems is, is there a limit to sort of throwing more scale and more computation at these problems? You know, you've gone from these early natural language processing systems to GPT-2 to GPT-3, and we're seeing these kind of incredible gains from building bigger networks and using more data and computation. And some people are skeptical that we're going to continue going all the way with this. And other people say, no, maybe this is, you know, we can just keep going and going indefinitely. And so I'm curious to hear your thoughts on that. Yeah. I mean, I think it is undeniable that we will go extremely far. I think that the current wave of progress with this specific paradigm has not ended yet. I think that's definitely true. I do think that there may be some capabilities which would be, for which it's not clear how this specific approach will give rise to. An example of that would be like, suppose we were to scale the GPTs much, much further, as is without any modification. If we were to allow modification, the argument would break down. But suppose no modification at all, just keep scaling, computing data. There was a blog post recently that I read. Someone was making the argument that would the GPT that would be produced by training and even more interesting data, be able to beat the world champion in chess. Probably not because, you know, the answer depends on whether someone has accidentally uploaded a lot of super high quality chess data to the internet. If no one has done that, then we shouldn't expect the GPTs to be good at chess, even if they were scaled up a lot. So I think they will be incredible, but there is a good chance that there will still be some gaps you'll need to address. This is a tricky question, but how do you view the difference between what a system like GPT-3 is doing and what the human brain is able to do? Because it seems like GPT-3 can learn to do just about anything that involves, okay, you have a series of, you know, tokens of text, and you want to predict what text comes next. And there are many problems that can fit into that, whether it's writing poetry or essays or answering questions or being a chatbot. But it does still seem like there's some things that humans do that just aren't going to fit into that paradigm. Yeah, I think there are definite things that you can point to about the human brain as being more efficient in multiple ways than the GPTs. And I'll give some examples. Some examples are from the way we can point at how people learn. And a different example would be from the observable behavior of people. I'll start with the second one because I feel like it's more, it's very easy to see. If you look at the capability of a GPT, it's very non-human in the following sense. A neural network that's been trained to guess the next word on a very large corpus of text on the internet will have a lot of familiarity with nearly everything. It will be able to speak pretty well about any topic, essentially any topic that's been discussed and talk about imaginary topics as well. It will have an incredible vocabulary. So, I mean, it's hard to imagine there is an English word that a GPT model wouldn't know and not be able to use. And yet we also know that it makes mistakes that humans would never make. Whereas if you compare it to a human, I think human beings have far smaller vocabularies. And the set of topics that human beings know seems to be far smaller. But human beings seem to know those topics far, far more deeply. I think that's like a real and a meaningful distinction. There's a sense in which a system like GPT-3 is already super intelligent in certain ways, right? Like as you point out, its vocabulary is essentially super intelligent and just its knowledge base, right? It just knows, in some sense, it knows about way more topics than you could ever learn in your lifetime. That's exactly right. And yet the depth that at least GPT-3 exhibits is lesser than the depth that the human has on, you know, if a human that studies one topic can achieve more depth than a GPT-3 of today. Of course, we expect future GPTs to increase their depth as well, but that seems like almost like a qualitative difference between these neural nets and humans. And relatedly, you know, related to the earlier comments about human beings not liking, really not liking memorizing information, human beings are also extremely selective about the information they consume. Extremely selective. You know, if you look at how you train a GPT, you just say, yeah, like give it random web pages and it's going to keep getting better. Whereas for human beings, are you kidding? If you give a person lots of random texts to read, well, and suppose somehow they've motivated themselves to do so, it's not clear that they'll benefit much at all. So I think that this is another very important way in which there is a difference where people are so choosy about the data they consume. And in fact, they will go to great lengths to find exactly the data that they need in order to consume. And I think this points to another difference. Now, this doesn't mean that pushing further on the GPTs will not lead to further progress. That's not the case at all. But I think it does mean that it is still possible to do better. There's also an efficiency issue here, right? It seems that a human can learn with way less information than GPT-3 can. Well, that one I think is a tricky one. I think there is truth to it, but I think it's tricky for the following reason. So the right comparison to make is how quickly do people learn at the age of 20, let's say, and how quickly does a GPT learn after you finish training it? Because to get to the age of 20, a person is exposed to a lot of concepts and information. They've learned a whole lot. A GPT model, because it doesn't have the benefits that biological evolution has given us in terms of various instincts and knowledge of what's the important data and what's important to focus on, we compensate by giving our GPT far more data. And the question then is asked, how quickly can a GPT learn once it's been trained on all this vast amount of data? And we know that at that point, the neural networks learn far faster than they are at the early stages. So the gap between what people normally think about the speed of learning in deep learning of artificial neural networks and the kind of GPT systems that we have today after we finish the training is significant. We learn much faster than it seems, but I do think that there is truth to the claim that human beings learn even faster still, and this is probably another gap. Though it is interesting to see what will happen to this gap as we continue to make simple and straightforward progress to GPTs through scaling them up and through making efficiency improvements.Two common critiques you hear about the standard neural net paradigm is one that sort of symbolic thinking or reasoning is missing from today's systems. And another is that some kind of embodiment in the world and interaction with the world may be necessary to learn the way humans learn. And so I'm just curious to hear your thoughts on those two critiques. So on the first critique, I feel like there has been quite interesting recent work. It's been trending on Twitter. I think it's from Google that show that if you simply take a GPT-3 and rather than ask it to answer some question, you ask it to use reasoning to answer your question or show your step-by-step reasoning when you answer this question, then the new, then GPT-3 will in fact generate step-by-step reasoning, perfectly good symbolic reasoning and get much, much better results on the kinds of tasks where symbolic reasoning seems to help like math. So I think there's, this is some evidence that maybe the current approach gets us at symbolic reasoning, or at least there is, maybe we should expect to make some progress more deeply. You know, the human brain is a neural network and it's the all perfectly capable of symbolic reasoning. So why shouldn't an artificial neural network be fundamentally incapable of symbolic reasoning? Do you think that there's enough in common between the neural nets we're implementing in computers and neural nets in brains that we can be confident that there's not some kind of qualitative difference there? I think we can't be sure. I think it's definitely conceivable that the human brain is doing something that's quite a bit better than our artificial neural networks. And I think what it will mean primarily is that it will mean that the amount of compute that will be required to reach to human level intelligence will be larger than it seems, than one might guess today. Because basically, if you think about a neuro, a biological neuron, there was a neuroscience paper from a few years ago where some people took the most sophisticated model of a biological neuron they could find, and they tried to approximate it with an artificial neural network. And they were able to do a really good job with a neural network, which had 100,000 connections. So then you could say, okay, well, if you replace each neuron with this little gadget made out of artificial neurons with 100,000 connections, now you have a giant artificial neural network, which is actually very similar to the brain. Now you need even more compute. So I would say that rather than things being fundamentally different, I think it's more a matter of degree in terms of how much compute would be needed for the artificial neural network to get to the point where it is in some sense comparable to a human brain. Right. I wonder if this is in some way a rephrasing of the idea that neural nets are function approximators, and they can kind of approximate any function. And so even if a human neuron does something really complicated, as far as we can tell, there's no particular reason that that couldn't be approximated to an arbitrary degree of accuracy by a neural net. So there is truth to what you say on some level. Universal approximation of neural networks basically means that if you have a neural network with a single layer, so it's a shallow network, but the layer is exponentially wide so that you have a neuron for every possible bit of space in your input space, then this neural network can approximate any function. And that's just wildly unrealistic, right? That's right. It's not relevant to anything, but there is a sense in which there is some kind of universal approximation going on, but of a different kind where you can say, hey, like you have your neural network, and suppose you have a neural network, and I'm saying to build something intelligent, we need to have some kind of computational gadget inside, you know, you need to use some kind of computational gadget. Well, the neurons inside the neural network can organize themselves to simulate this computational gadget. Do you see what I mean? So then suppose we say, hey, like you need some kind of special operation inside a neural network. Well, the neurons in the neural network can say, no problem, let us organize ourselves using training as to implement that precise operation. And suppose we say you actually need biological neurons. Well, you can literally say, hey, let's imagine a truly gigantic artificial neural network where groups of neurons with 100,000 connections between them, each corresponds to a biological neuron. And now this whole system can simulate a large number of biological neurons. Right. Maybe instead of calling it universal approximation, it might be better to call it universal simulation. Right. It can essentially approximate any algorithm that's needed. Yes. So the other critique that comes up sometimes is that AIs may need to be embodied in the world and kind of like interacting with the world in order to one day learn to do all the things humans can do. I mean, certainly there is an argument to be made since humans interact with the world, and they are embodied, and they are our only example of intelligence. I think that possibility exists. I think it's also quite likely that it's possible to not be physically embodied to compensate for the lack of embodiment with the vast amount of data that exists on the internet. So probably my bet is that physical embodiment is not necessary, though there is some chance that it will make things a lot easier. Some groups like DeepMind, I know, have been experimenting with putting AI in kind of simulated worlds, which is sort of like somewhere in between, where it's not putting it in the real world, but it is putting it in a world where it can do things and then see their impacts and learn from that. Yeah. Taking action seems important, but doing it in a physical form factor may be less so. Could the act of answering open-ended questions about yourself give you new, important insights? Turns out the answer is yes, if those questions are selected in just the right way. After running a series of five scientific studies, ClearerThinking has discovered a specific set of practical, yet rarely asked questions that 83% of people reported were valuable for them to answer, and 78% said they would recommend to others. A remarkably high 88% of people even reported that they enjoyed answering these questions. And ClearerThinking is now making those questions available to you for free on ClearerThinking.org so that you can benefit from them as well. You can also order a beautiful physical card deck of the life-changing questions so that you can use the questions to bond with friends and family. We think you'll be surprised just how valuable answering these open-ended questions about yourself can be. To answer the free life-changing questions or to find ClearerThinking's other free tools and mini-courses, head to ClearerThinking.org. So I'm curious to know, before we wrap up the conversation, how much do you worry about potential dangers from AI? So I would say the questions around AI and the challenges and dangers that it poses vary by timescale. As capabilities will continue to increase, the power of AI will become greater. And the greater the power of a system is, the more impact it has. And that impact can have great magnitude and all kinds of directions. And I would say it could have a positive direction.or it could have a negative direction. You know, when we started OpenAI, we already sensed that AI, once sufficiently developed, could indeed pose a danger because it is powerful. And although with new technologies, it is very hard to predict how things will unfold, it seems desirable to, at minimum, be thinking quite hard about the different ways in which AI, once it becomes very, very capable and powerful, could be used in truly undesirable ways. Or, as in the case of AI, AI is a special technology where if it is built incorrectly, if it is designed without sufficiently care, then it could lead to outcomes nobody wants without anyone's intention. So yeah, I spent quite a bit of time thinking about it. I tend to divide the potential dangers from AI up into three categories. One is misapplication or bad application of narrow AIs, like we have today. Like, for example, someone using a large language model to generate sort of endless combinations of spam that maybe spam detectors can't detect, or narrow AIs today maybe having racial bias because they're trained on data that has racial bias or something like that, right? So that's like kind of narrow dangers. The second category I think about are ways that AI could enhance the power of one group over all other groups. Like an authoritarian regime using AIs that monitor every person in the population or quadcopters flying around, watching everyone all the time, and even assassinating people if they misbehave. And then the third category of AI dangers I think about are potential dangers from uncontrolled AI, like AI, potential AI superintelligence. If one day we make AIs that are really smarter than humans in not just narrow ways, but in many ways or even every way that those systems themselves, if not properly controlled, could cause great danger. So I'm just curious to get your take just on these three categories. So, you know, starting with category one on dangers of narrow AI, what are some things that you're concerned about and what do you think we can do to try to protect against that? All the dangers that you mentioned are extremely valid and we are already facing the first danger that you're describing today. It is indeed the case that if you train a neural network on data which exhibits undesirable biases and then you do nothing to address somehow inside the neural network, then the neural network will exhibit them. And so this is a problem that the field of AI today is very keenly focused on. And I think that there are multiple things that one could do in order to mitigate this concern, this danger. So the first one would be around, this is something that we do at OpenAI, where we don't just release these models into the vial. We expose them through an API and we carefully monitor how the API is being used and we carefully restrict the allowed use cases. So this is kind of a semi-manual solution, but it is effective. We say, okay, this use cases, like if a particular use case is going to expose a lot of surface area for bias, then we should not allow it or other kinds of negative use cases. And if a different use case is okay, then we should allow it. And then of course, all this on top of further training of the model to then learn to not exhibit the highly undesirable biases. And there is a lot of work about it in the field and there is quite a bit of awareness of this problem as well. So that would be the thing I would say about the first problem. Yeah, and I really like that approach because it gives you feedback, right? Because you have this API, you're able to monitor how people are using it and then you're able to notice patterns in misuse and then update your system to detect them. So that seems really valuable to me, but it does make me concerned about copycats, right? So even if you have the absolute cutting edge AI and you monitor it really carefully, others copy the work you do. And even if they're a year or two behind you in their development, because you're the best at what you're doing, it just means that people can use these more dangerous applications on a model that's one to two years out of date. And so I'm just wondering how you think about that. Yeah, I think this one is definitely a trickier one. The issue that you're describing is eventually technology is going to be diffused and there'll be lots of different companies going to implement it. That's just a true point. Yeah, it's tough because it's a kind of a collective action problem, right? That's right. But one thing that OpenAI has tried to do, we've tried to implement some self-regulation. We recently, together with Cohere and AI.1.2, which are all three large language model API companies, have came together to make a statement about shared principles about how these models should be used and how they should be deployed precisely to address the risks you're describing. And the hope is that other entrants into the space will follow our example and they will also follow those principles. That's great to hear that you're trying to solve this collective action problem by getting people together, especially the groups that are sort of cutting edge and trying to all come to agreements on what you all want to enforce together. Of course, you can't bind the most nefarious actors who are just trying to make a buck and don't care about principles, but at least you can get the major players on board. That's right. And if you get the major players on board, you also will get the great majority of volume of use on board as well, if you see what I mean. At least that's the aspiration and there is a story that it will be the case. One thing I'm worried about is whether people would use these natural language models to generate sort of customized spam or customized attacks. So an example would be, we know that certain countries will kind of manipulate Twitter and have warehouses of people that will be posting to try to create propaganda. We also know that around the world, there are people that sort of do custom attacks to individuals in order to compromise their computers, phishing attacks and so on. And you can imagine that the natural language processing systems are getting so good that they might actually be able to automate these where it's instead of a thousand people in a warehouse, it's 10 million automated bots all saying actually different things, but sort of all espousing some propaganda in a particular direction or doing customized phishing attacks based on what's known about a person. So I'm just curious if you've seen any evidence of those kinds of attacks occurring or what your thoughts are about trying to prevent them. Yeah, so I have two thoughts on this. Like the first one is, when we first looked into releasing GPT-3 through an API, this use case in particular was one that we've really worried about. We really worried about people using GPT-3 to create propaganda and to make the kind of persuasive arguments in a particular direction in order to manipulate people or to persuade them. We thought this would be a major use case. Empirically, it hasn't been the case. We were looking for it and we weren't able to find it. So that's good. It doesn't mean it won't emerge in the future. It may mean, for example, that countries who engage in these kinds of activities, they have sufficiently cheap humans. So maybe that's a good thing.Maybe that scale up, maybe the cost benefit isn't quite there yet for them. Maybe it will be with better GPTs in the future. But I do think that in addition, there is one possible dynamics. I don't know if it will happen, but I think it could happen and there is a plausible story for it happening and I'd like to mention it. So one thing which I think will hopefully be true is that the, let's say, I think the attempt for the AI industry to self-regulate will be successful to a significant degree. And so we should expect that all the companies with the best models should be on board with it. And there should be perhaps systems from like a couple of years behind who will be used for these nefarious purposes. But I would hope that at that point in time, like for example, Google, or let's say, will be able to deploy a massive persuasion detection system in their email or something. It seems totally within reach as well for that particular concern which you're describing. I mean, I don't, I don't know if it's, I wouldn't say it will happen right away, but I could imagine that once that starts to be an issue, once indeed there is, there are indeed bot attacks of this kind, there will be a lot of interest in using similar neural networks to detect the application of such personalized manipulation. Yeah, I mean, an important advantage in having the cutting edge systems be in the hands of groups that are more responsible is that maybe that means that they can actually detect the less cutting edge systems, right, that are one or two years behind there. You know, maybe GPT-3 can fool itself, but maybe it's able to tell that GPT-3 is generating something. If you see what I'm saying. Yeah, exactly right. I mean, especially, so especially if the more powerful system is trying to recognize if less powerful systems are trying to do something that there's definitely a story that they should be successful, or at least they could be successful. The second big category of AI dangers that I think people are concerned about is a concentration of power, where if you have an AI system that's really, really good, it may enable some groups to just gain a huge leverage over other groups, or maybe even over the rest of the world. And you know, some examples of this would be like, if one AI created by far the best financial trading system ever, and was able to, you know, make not just billions of dollars, but trillions of dollars, right? Or another example would be if AIs can get so smart that they can do automated hacking, and can, you know, you can have essentially AI hackers, you know, millions of AI hackers hacking every system simultaneously around the world. Or even just making predictions, if you have AI systems that can make predictions far better than humans can about many different things, this could lead to one group having a huge power differential. Or I'll just give one final example would be, if you have AI systems that can replace human labor, and, you know, one company could have, you know, 100 million AI laborers, essentially, and replace, you know, large swaths of human labor around the world, but it's all owned by one company, right? So all these examples are essentially concentrating power in a way that could potentially be scary. So I'm curious to hear your thoughts on this. Yeah, I think there's a lot of truth to the underlying idea that AI is a concentrating technology simply because of the need of a very large cluster. So the biggest AI systems will always be the most capable and most powerful. I would say that there are two questions you can ask, whether there is an overall increase in the power in AI systems, power that AI systems wield, I think the answer is definitely yes. And this power is going to be increasing because the systems will become more capable, will continue to become increasingly more capable over time. You can make a reasonably good, a reasonably strong case that the various capabilities that you mentioned will be increasing continuously. And the reason you see that is because we have some degree of this continuity today. Like, for example, you know, we discussed GPT-3, and there are all those things it can do, just not very well, and you wouldn't want to use it. It kind of gives you like a taste. You could say, well, one day it could do all those things, but right now you wouldn't want to use it. I mean, even in the self-driving car, kind of see what it will, how great it would be. It would, it worked, but it really doesn't work at all, right? I mean, it kind of works, but also doesn't work right now. So I think we will see this kind of gradual increase where as the capabilities increase, people will find new ways to use them in order to grow the economy, the productive ways, or in harmful ways, like you mentioned, hacking or some other ways, manipulation. I think as capabilities increase, people will find new ways of using those capabilities to advance their goals. So I think this is an argument for some degree of continuity, but yeah, I think there's definitely a real concern that AI could lead to more concentration of power than is desired. And I think it may be that society will need to, well, there may need to be a discussion in society perhaps ahead of time of how this should be dealt with. My understanding is that open AI has kind of a capped return for its investors, right? I think it's a hundred X return. And then after that, the idea is to kind of give back any further profits to humanity. Is that right? So that's right. The thing that's more, the thing that's the underlying idea there is that, you know, if you have really significant AIs creating an incredible amount of wealth, actually incredible amount of wealth, like if you run into the future a little bit and you allow yourself to imagine the sci-fi being materialized, the sci-fi ideas of AI being materialized, then the world will look very different. And it seems desirable. It seemed desirable to us to at least have the option to not be forced to maximize revenue at the absolute largest speed possible, or at least to have one fewer powerful incentive to do so. The future is hard to predict. I don't know what will happen. Maybe it's, maybe it's all going to be fine. Maybe it's going to be like, you know, you have multiple competing AI companies. The price of the services drops to zero, their profits are all very small. Maybe that's fine. But it seems desirable to have at least, well, we don't know what's going to happen. So at least let's not be forced to go down this route with absolute force. If we can just avoid it, let's not make it so that we are absolutely forced to maximize revenue at the highest speed possible. Let's us have the ability to not do so if it seems like the best course of action. Yeah, it seems like maximizing profit is not the ideal when you're, when you're talking about the potential for these technologies, right? If we're just talking about incremental gains, okay, making profit, fine. But if we're talking about really something transformative, where potentially hundreds of millions of jobs or billions of jobs are getting replaced with AI, the effects on society could be so great that we, we need to proceed with extreme caution. Yeah, pretty much. We are venturing into very, we are moving into very unknown territory, and it's going to be tricky to navigate. So it seems like, it seems net positive to have more, more degrees of freedom in your actions and say, hey, like, if we can slow down our growth strategically, because it will lead to some better outcome, we should be able to do so. So we only have a few minutes left. So the last thing I want to ask you about is some of the concerns around this third topic, which is the idea of one day, we may build an AI that is really...more intelligent than humans, or at least so intelligent that we can really think of it as a general intelligence and not just a narrow intelligence. My first question around that is your reaction to the effective altruism community, because this is something the effective altruism community talks a lot about, about the potential dangers from such an AI. And I just wonder how you feel about their critiques. Yeah. I mean, I have to say that this is an example where the community has done some very foresightful thinking about the problem far ahead of its time. There is a possibility that, there is definitely a possibility, like, so let's, let's look at the, at the, at the situation, right? What is the statement? The statement is, as you said, if you have a truly super intelligent AI, what are the dangers that it poses? Not just people misusing it, which is going to be already a tricky question to navigate, but risks from the technology itself. And the thing which I will say is, I think it is definitely a positive and a productive thing that the community has done to bring awareness of this question into the minds of more mainstream AI researchers. I think those questions have the potential to be very real and very important. There is the possibility that things will work out. The possibility exists because there will be unexpected discoveries in ML. There'll be radical changes in our understanding as they have happened before. But there's also a possibility that indeed those AI systems will be very, very tricky to handle when they're super intelligent. Because if you take seriously the idea of a super intelligence, it's one hell of a thing. It's a very, very potent object. And this is a question, if you think about a lot at OpenAI and how to deal with this and how we should do research about it and the strategic implications still work in progress. But yeah, it's not possible to think seriously about AI without thinking about AI and where it's going without thinking about these questions as well. Yeah. And I think a critique that I've heard some people in the EA community make is that if these systems have such potential for danger, then we can't responsibly try to build them at all. That, like, until we figure out how to build them safely, we should not build them. Or at the very least, it should be a collaborative effort where lots of groups work together and take extreme caution and really make sure to avoid any kind of race dynamics where people are racing and rushing to beat each other to build it. So I'm curious to hear your reaction to that. I'll comment on this question rather than answer it by saying two things. The first is that indeed it is a very tricky situation with AI. But I can mention to you the evolution that we've undergone in our own thinking inside open AI, and I expect that many labs will go through a similar evolution in their thinking as well. When we started open AI, we thought that open sourcing would be a very good path forward because you had these big companies and they would be able to concentrate all their progress and would open source it. And that would be a way of dealing with this concentration of power that the natural progress in AI would lead to. But then we realized that actually that's not the way to go. Once we truly internalized the power that AI systems will have, it became clear that just open sourcing the technology is no longer the way to go. Instead, the way to go is something much more careful, which is the approach we've taken with the slow releases of our systems through the API, through an API, which is slow, deliberate, careful, and that will continue to be more the case as we continue to make progress. I expect that as the different labs will fully internalize just how capable AI will be, then there will be a lot, then these concerns will propagate into all the labs at the frontier. And so I think there is a reasonable case to be made that care will emerge. But the other thing is that ultimately it's not possible to not build AI. And the thing that we have to do is to build AI and do so in the most careful way possible. Yeah, I think one of the biggest concerns that I have personally is that there will end up being a race as we get closer. And it seems to me that anything that can be done to prevent that is great because it means that if you're not racing, then you can take your time, right? You can really think about risks, you can really be careful. Are you optimistic that as AIs get more and more capable, that the top groups really will work together instead of filling competition with each other? I definitely feel that for the Western groups, there's a pretty good chance of that, yeah. That's great to hear. Ilya, thank you. This was a fascinating conversation. I really appreciate you taking the time. Yeah. Thank you so much. It was a pleasure. Thanks again for listening. We always love to hear from our listeners. So if you have questions or comments for us, just send us an email at clearerthinkingpodcast at gmail.com. This episode was edited by Ryan Kessler and transcribed by Janessa Burrill. Uri Bram is the podcast's factotum. To find show notes, transcripts, and more info about the show, visit clearerthinkingpodcast.com. And if you like the show, we'd really appreciate it if you could rate and review us wherever you get your podcasts and tell your friends about us on social media. We also hope you'll subscribe to our email newsletter called One Helpful Idea. Each week, we'll send you one idea that we think is really valuable that you can read about in just 30 seconds, along with that week's new podcast episodes, an essay by Spencer, and announcements about upcoming events. You can sign up for that newsletter on our website, clearerthinkingpodcast.com. A listener asks, you've mentioned in a number of episodes that causation is important. Why do you think causation is important? For example, evidential decision theory, or EDT, doesn't use explicitly causal information at all. So what do you think is wrong about that? I think I have a slightly controversial opinion about causation, which is that I don't think it's fundamental. Like my, well, I'm not very confident, but my best guess is that causation is not fundamental. Causation is like an aspect of a model. So causation comes into play when you start asking questions like, well, what would have happened had things been different, right? But like, that's not the way reality works. Reality just like does its thing. And then models have this property that you can ask a question about them, like what would happen if things had been different, right? So I think causality is like an element of a model. And then the reason I think it's really important is because we want to do things in the world. We want to make the world different than it would have been had we not existed. And so when you're in that frame of wanting the world to be different than it would have been had you not existed, then you automatically are concerned with causality because that's what causality is. The aspect of the model lets you ask the question, well, what do I need to do to achieve this different outcome than would have occurred? But I don't put too much back in evidential decision theory. I feel like it doesn't perform that well in some of the weird philosophical thought experiments. So I don't abide by it particularly. Thank you. Thank you. 